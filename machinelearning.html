<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Machine Learning</title>
        <link rel="stylesheet" href="styles.css" />
   		<script src="https://lis500-egbecker.ischool.wisc.edu/p5.js"></script>]
		   <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/latest/p5.min.js"></script>

    </head>
    <body>
        <header>
            <!-- using buttons in the nav bar for look and feel -->
            <ul>
                <li><a class="btn" href="https://lis500-egbecker.ischool.wisc.edu/">Home</a></li>
                <li><a class="btn" href="aboutus.html">About Us</a></li>
                <li><a class="btn" href="techhero.html">Our Tech Heroes</a></li>
                <li><a class="btn" href="resources.html">Implicit Bias Resources</a></li>
                <li><a class="btn" href="machinelearning.html">Machine Learning</a></li>
            </ul>
        </header>
        <h1>Machine Learning</h1>
        <p>
            Machine Learning (ML) is a subset of artificial intelligence (AI) that involves training computer algorithms
            to recognize patterns and make data-driven predictions or decisions without being explicitly programmed. At
            its core, ML enables systems to learn from past experiences, improving their performance over time as they
            process more data.
        </p>
        <p>
            The process begins with data collection, where raw datasets are gathered and prepared for analysis. This
            step often includes cleaning the data to remove errors, inconsistencies, or irrelevant information. The next
            step is feature selection, where key variables or attributes in the dataset are chosen to enhance model
            accuracy.
        </p>
        <p>
            Once the data is prepared, an ML algorithm is selected based on the problem type, such as classification,
            regression, or clustering. Algorithms like decision trees, neural networks, or support vector machines are
            then trained using a portion of the dataset, enabling them to identify relationships within the data.
        </p>
        <p>
            During training, the algorithm adjusts its parameters to minimize errors, improving its predictive
            performance. After training, the model is validated and tested with new data to ensure reliability and
            generalization. Finally, the model can be deployed to make real-world predictions or decisions, with its
            performance monitored over time for continuous improvement.
        </p>
        <h1>Our Project Statements</h1>
        <h2>Liz's Statement</h2>
        <p>
            When David and I set out to build a machine learning model using Google's Teachable Machine, we immediately
            had a clear vision: to teach a computer how to recognize common jazz chords like Major 7, Minor 7, Dominant
            7, and Minor 7♭5. As two music enthusiasts (I have a 6+ year choral background, which is very meager in
            comparison to David's experience with music), this seemed like a perfect blend of our passions. However, we
            quickly learned that teaching a model is much more complex than it looks—and much more rewarding when things
            finally work.
        </p>
        <p>
            Our original goal of chord recognition proved unexpectedly challenging. Teachable Machine could only handle
            one label at a time, while chords combine multiple pitches. After several troubleshooting and data
            collection rounds, we realized there were better goals than this. Instead of giving up, we pivoted and
            decided to teach the model something more feasible: recognizing the 12 notes of the chromatic scale.
        </p>
        <p>
            David recorded audio samples of each note, adjusting for different tones, dynamics, and playing styles.
            After training the model with these sounds, we watched in amazement as it correctly identified pitches in
            real time. This success felt incredibly satisfying after the struggles we had faced earlier. The experience
            taught us that model development is more about adapting to limitations than technical know-how. This
            adaptability in the face of challenges should inspire you to stay resilient and determined in your own
            projects.
        </p>
        <p>
            One of the most valuable lessons came from reflecting on how we built our dataset. After trying several
            times to figure out what audio would be the best to train our model on, we realized our initial dataset was
            too narrow. We expanded it by adding more samples and recording in different environments. This process of
            data expansion was crucial, as it helped us appreciate the complexity of training even simple models and the
            importance of data diversity in machine learning.
        </p>
        <p>
            While this project focused on music and sparked deeper discussions about AI's broader impact, Joy
            Buolamwini's <i>Unmasking AI</i> opened my eyes to how machine learning can perpetuate bias if not designed
            carefully. Though our project did not involve sensitive data, we still thought critically about fairness and
            accuracy. How do our choices when creating datasets shape what AI can and cannot do? This question became
            central as we refined our project and considered how transparency could benefit future developers.
        </p>
        <p>
            Our project was a genuine team effort. Combining musical knowledge, coding skills, and a shared interest in
            design, we built a working webpage where users could test the model. The site's look and functionality
            mattered as much as its technical performance. We wanted visitors to understand and enjoy what the model was
            doing, so we embedded interactive demos and created a user-friendly layout. This collaborative effort was
            key to our project's success and should remind you of the value of teamwork in your own endeavors.
        </p>
        <p>
            Looking back, we learned that machine learning is accessible but far from simple. It is a continuous cycle
            of testing, learning from failure, and improving. The technical work was rewarding, but the more significant
            takeaway was realizing how machine learning projects connect creativity, ethics, and collaboration. Every
            dataset, design choice, and piece of feedback shaped our project in unexpected ways.
        </p>
        <p>
            As we move forward, we will carry these lessons with us in future AI projects and how we think about
            technology's role in the world. AI is not just about building intelligent machines but about making
            thoughtful, transparent, and creative choices that push the limits of what is possible. This project
            reminded us that success in AI comes not just from technical skills but curiosity, adaptability, and a
            willingness to learn from every unexpected challenge.
        </p>
        <h2>David's Statement</h2>
        <p><strong>All 12 Notes but no Color; Employing Google&rsquo;s Teachable Machine with Musical Datasets</strong></p>
<p>We programmed our teachable machine to identify the 12 chromatic pitches of Western music. Play, sing, or whistle a note into your microphone and the machine will correctly identify it; When asked to perform higher-level musical analysis, the recognition of chordal quality, the machine failed. The journey to create this teachable machine was interesting and exposed some truths about my biases and those inherent in the machine. Through this process, understanding how the various problematic and harmful examples of AI algorithms and datasets presented in Joy Buolamwimi&rsquo;s <em>Unmasking AI </em>so easily found their way into common use. As Buolamwimi herself relates, &ldquo;as a computer scientist, I was trained to focus on the technical, not the ethical.&rdquo; The lack of ethical considerations is only compounded by the pressures of the competitive rush to be first in research and the marketplace.</p>
<p>&nbsp;</p>
<p><strong>Music Theory Background Information</strong></p>
<p>Our original plan was not to create a machine that identified single pitches accurately, but one that could identify the quality of the 4 types of chords most commonly found in Jazz/ Black American Music; Major 7, minor 7, Dominant 7, and Minor 7 flat 5 chords. Unlike isolated individual pitches, each of these chord types has a distinct sound that most students of contemporary music can identify without other contextual clues. &nbsp;Chordal quality is an important aspect of both <a href="https://iastate.pressbooks.pub/comprehensivemusicianship/chapter/6-1-diatonic-harmony-tutorial/">functional</a> and <a href="https://www.berklee.edu/berklee-today/fall-1999/Nonfunctional-Harmony">non-functional</a> harmony and is often compared to the use of color in the visual arts.&nbsp; It is determined by the intervallic relationships between the four notes of the various types of seventh chords. These notes are the first, third, fifth, and seventh scale degrees. For example, here are all the notes of the C major scale:</p>
<p>C, D, E, F, G, A, B</p>
<p>The C major 7 chord would be just the &nbsp;4 notes C, E, G, and B.&nbsp; These notes are all separated by an interval of either a major or minor third.&nbsp; In the case of a major 7 chord, the intervals are as follows:</p>
<p>C to E, Major third</p>
<p>E to G, Minor third</p>
<p>G to B, Major third</p>
<p>We can create a chord starting from each note of the same scale, and get at least one of each of our 4 chord types from any major scale. For example, if we arrange all the notes of the C Major scale to start on D, we get the following:</p>
<p>D, E, F, G, A, B, C</p>
<p>This is considered to be the mode of D Dorian, and from it we can derive a D Minor 7 chord, using the notes D, F, A, and C; The intervals between these chord tones are as follows:</p>
<p>D to F, Minor third</p>
<p>F to A, Minor Third</p>
<p>A to C, Minor Third</p>
<p>If we arrange the notes starting on the fifth scale degree of C major, the G note, we get the G Mixolydian mode:</p>
<p>G, A, B, C, D, E, F</p>
<p>The corresponding G Dominant 7 Chord, spelled G, B, D, and F, is structured on the following stacked thirds:</p>
<p>G to B, Major Third</p>
<p>B to D, Minor Third</p>
<p>D to F, Minor Third</p>
<p>To get a Minor 7 flat 5 chord, we need to start at the 7<sup>th</sup> scale degree of C Major, thereby attaining the B Locrian mode:</p>
<p>B, C, D, E, F, G, A</p>
<p>The B Minor 7 flat 5 chord, spelled B, D, F, and A, has the following intervallic relationships:</p>
<p>B to D, Minor Third</p>
<p>D to F, Minor Third</p>
<p>F to A, Major Third</p>
<p>When learning to identify chordal qualities, students are often encouraged to find feelings or sensory adjectives that each chordal type seems to convey to them personally. I hear Major 7 chords as cool, Minor 7 chords as dark and warm, Dominant 7 chords as bright, and Minor 7 flat 5 as cold and mysterious. These feelings associations are not uncommon, and there is literature that uses similar adjectives to describe the <em>feeling </em>of these chordal qualities similarly. There have even been some <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5717663/">questionable attempts at codifying the emotional content of chordal qualities</a>. &nbsp;These feelings are likely more reflective of the values and traditions of the dominant culture than they are universally true. There are plenty of examples outside and inside Western music of harmonic structures being associated with music that runs antithetical to this, such as the Blues, which in its most essential form is built almost exclusively on dominant chords, and the popular and folkloric music of the Balkans and East Africa, which are almost exclusively built on minor modes, yet are often songs of celebration in the former or spiritual praise or romantic longing in the later. Regardless of the specific associations, you would be hard-pressed to find anyone who doesn&rsquo;t have some type of feeling associations with these rather complex chordal structures, thereby making them easy to identify through emotional imprint, even for the least knowledgeable of students.</p>
<p>Alternatively, identifying individual pitches in isolation is very difficult, even for the most seasoned of professional musicians and music educators.&nbsp; This ability, often called perfect pitch, is very rare in humans and imperfect, at best.&nbsp; Most human musicians rely on relative pitch, which is more about hearing and understanding the relationships between harmonic and melodic intervals than about identifying them in isolation.&nbsp; A skilled musician with good relative pitch can learn a melody or a harmonic sequence from a recording without touching a musical instrument, but they will need to play their instrument as a point of reference to determine what key the music is in. Perfect pitch can actually prove detrimental to some musicians, as their hearing can change over time, <a href="https://www.youtube.com/watch?v=3rx08qWtFak">and they can begin to hear things out of tune</a>.</p>
<p>The Teachable &nbsp;Machine can easily identify an individual pitch or note because it simply has to identify a specific frequency and then respond with the name of that frequency as it was taught.&nbsp; While the timbre of different instruments or voices will color the note differently, the fundamental pitch is always the same, as are the potential harmonics.&nbsp; &nbsp;Identifying pitches this way is easily accomplished with much simpler technology than this teachable machine.&nbsp; A great example of this is an <a href="https://www.fwordmag.com/single-post/how-does-a-guitar-tuner-work">electric guitar tuner</a>.&nbsp; Technology like this has its foundations in the <a href="https://en.wikipedia.org/wiki/History_of_the_oscilloscope">earliest oscilloscopes</a>, which date to the late 19<sup>th</sup> century.&nbsp; The Google Teachable machine analyzes waveforms, similarly to the oscilloscope or a guitar tuner. The big difference is that the machine applies meaning to that information, in this case, that meaning is a note name.&nbsp;</p>
<p><strong>Data sample creation process</strong></p>
<p><strong>Pt 1: The Chord Quality Machine</strong></p>
<p>Operating from the perspective of a human musician who does not have perfect pitch I thought that building a machine that could learn what different chord qualities sound like and identify them would be relatively easy.&nbsp; Using my vibraphone, audio interface, and one Blue Bottle microphone, I was ready to capture some audio samples. I decided to put some basic parameters in place to build the initial data set. I went through several unsuccessful iterations, below are the parameters for the final version I created:</p>
<ol>
<li>I created 5 classes for the Teachable Machine; the 4 types of chords and a background noise class.</li>
<li>I recorded all 4 types of chords in root position (1,3,5,7 chord spelling) in all 12 keys. I decided not to record other common voicings of these chords or add harmonic tensions, even though in actual musical practice these would be commonplace.&nbsp; I was determined to get the machine to first recognize these basic versions of the chords, and then add those variations later if it was successful.</li>
<li>Each chord was played by me on the vibraphone and recorded with the sustain pedal suppressed, allowing the chord to ring out. I recorded each chord for 8 seconds, which in turn were broken up into 8, 1 second samples. This results in a total of 384 samples in total.</li>
<li>The datasets were fed through the teachable machine for 300 epochs</li>
</ol>
<p>&nbsp;</p>
<p>Unfortunately, the machine misidentified the chord qualities nearly every time</p>
<p><strong>A Theory on Why the Chord Quality Identifier Failed</strong></p>
<p>&nbsp;My faulty assumption was that the aspects of aural training that humans find easiest to master would be the same ones that the machine could easily identify.&nbsp; This thesis proved incorrect. The teachable machine is powered by transfer learning, and built on<a href="https://github.com/tensorflow/tfjs-models/tree/master/speech-commands"> Speech Command Recognizer.</a> This model is built upon TensorFlow.js, and was designed specifically for simple speech recognition tasks:</p>
<p>&ldquo;Its primary goal is to provide a way to build and test small models that detect when a single word is spoken, from a set of ten or fewer target words, with as few false positives as possible from background noise or unrelated speech. This task is often known as keyword spotting.&rdquo;</p>
<p>Keyword spotting is the voice recognition used to call the attention of voice-controlled devices and applications, such as &ldquo;Hey Siri,&rdquo; or &ldquo;Hey Google.&rdquo; The human voice, under the normal circumstances of speech, only produces one pitch at a time on vowel sounds, and non-pitched articulations on consonant sounds.&nbsp; The machine&rsquo;s initial datasets were linear series of singular frequencies and non-pitched sounds, in 1-second groupings.&nbsp; The simplicity and lack of context of these pitches is what makes their recognition so challenging to human pitch memory. Similarly, the complexity of the chord qualities makes them very easy for human ears to classify, but nearly impossible for this machine-learning model to recognize as distinct classes.&nbsp; My assumption that machine learning would find the same tasks easy that humans do was totally flipped on its head in this process.&nbsp; The implications here suggest that the biases of both the Teachable Machine's creators and the members of the tech community using this and similar technology can lead to problematic and even dangerous outcomes. Taking a closer look at the creation of the Teachable Machine&rsquo;s underlying small language learning model, Speech Command Recognizer, reveals the potential and high probability of the ghosts of the bias in this machine.</p>
<p><strong>Data sample creation process</strong></p>
<p><strong>Pt 2: The Pitch Identifying Machine</strong></p>
<p>When the Teachable Machine failed to recognize chord quality, I wanted to find something it could do successfully. I inverted my thinking; if it failed at a task most musicians find easy, how would it do on a task most musicians find extremely difficult?&nbsp; &nbsp;&nbsp;Bearing in mind that the pitch memory of the machine was absolute so long as I had a class for all 12 pitches, I thought that success was highly likely.&nbsp; Here is how I created my datasets to teach the machine perfect pitch:</p>
<ol>
<li>I created 13 classes; one for each of the 12 pitches and one for background noise</li>
<li>For each of the 12 note classes I played the pitch in all available octaves on my vibraphone. The vibraphone is a 3-octave instrument, starting with the F above middle C and terminating at the F 4 octaves above that. This means that there are 4 F&rsquo;s, and three of every other pitch sampled, totaling 37 notes.</li>
<li>Each note was played by me on the vibraphone and recorded with the sustain pedal suppressed, allowing it to ring out. I recorded each chord for 8 seconds, which in turn were broken up into 8, 1 second samples. This means there is a total of 296, 1-second samples.</li>
<li>The datasets were fed through the teachable machine for 100 epochs</li>
</ol>
<p>&nbsp;</p>
<p><strong>A note on pitch and tuning:</strong> This machine, being based on the well-tempered system that all keyboard instruments are tuned to, does not account for the slight differences in pitch that a vocal or string ensemble would employ.&nbsp; For example, on a keyboard, the notes Bb and A# are enharmonically the same but will be slightly different than each other when performed by a vocal ensemble.&nbsp; For more on this please visit the Wikipedia article about musical temperament <a href="https://en.wikipedia.org/wiki/Musical_temperament">here</a>.</p>
<p>&nbsp;</p>
<p><strong>The Coded Gaze</strong></p>
<p><em>&ldquo;</em><em>In my work, I use the coded gaze term as a reminder that the machines we build reflect the priorities, preferences, and even prejudices of those who have the power to shape technology. The coded gaze does not have to be explicit to do the job of oppression. Like systemic forms of oppression, including patriarchy and white supremacy, it is programmed into the fabric of society. Without intervention, those who have held power in the past continue to pass that power to those who are most like them. This does not have to be intentional to have a negative impact. The task ahead of me was to see if I could find compelling evidence showing the coded gaze at work.&rdquo;</em></p>
<p>
Joy Buolamwimi<em>, Unmasking AI: My Mission to Protect What is Human in a World of Machines</em>
</p>https://lis500-egbecker.ischool.wisc.edu/p5.js
<p>In <em>Unmasking AI, </em>Dr. Joy Buolamwimi exposes and dissects the inherent bias found within the algorithms and data collection methodologies that power much of the AI that is currently being used for commercial and governmental applications.&nbsp; She is first exposed to the coded gaze when working on a graduate school project that requires her to employ code used for existing facial recognition systems. The system does not recognize her dark complexion as a face, and she is forced to use a white Halloween mask to use and test her project.&nbsp; Digging deeper, she uncovers that the datasets that the AI has learned from reflect the biases of its creators.&nbsp; As she unpacks this, it is discovered that this is a consistent problem across most of the machine learning systems in use.&nbsp; Throughout the book, Dr. Buolamwimi works tirelessly to expose the potential dangers it poses to underrepresented groups, particularly women and people of color.&nbsp;</p>
<p>In chapter 16 of the book, entitled <em>BROOKLYN TENANTS, </em>Dr. Buolamwimi is recruited to assist a legal team that represents the residents of Atlantic Towers, a Brooklyn, NY apartment complex.&nbsp; The management company that operates the building is planning on replacing the existing keyless fob entry system with a facial recognition entry system created by a company called StoneLock. The system uses the heatmap of an individual&rsquo;s face to allow entry to the building. The firm claims that it does not differentiate the gender, sex, or color of the person trying to enter the building because it &ldquo;merely reads data points on a person&rsquo;s face and assigns a number.&rdquo; The legal team asks Dr. Buolamwimi how this system may infringe on the rights of the tenants of Atlantic Towers, and possibly create difficulty accessing their homes.</p>
<p>The tenants had not agreed to having their faces used for biometric entry prior to moving into Atlantic Towers. They had no way of ensuring that their biometric data wasn&rsquo;t being sold or turned over to law enforcement. Dr. Buolamwimi&rsquo;s work had already exposed the lack of biometric information available about women of color, and that population was the large majority of tenants; &ldquo;The vast majority of the tenants belonged to one or more of the groups that have the highest failures in the U.S government-sponsored studies [<em>sic</em>] &hellip; that examined the accuracy of facial recognition technologies.&rdquo;</p>
<p>StoneLock didn&rsquo;t make clear the demographic and phenotypic makeup of the training tests they used to develop their product, and Black and Brown folks were underrepresented in the publicly available datasets at the time.&nbsp; Furthermore, there were specific challenges of the near-infrared facial recognition methods that StoneLock was using.&nbsp; The accuracy of the system could be compromised if people trying to enter using the system presented signs of physical or emotional fatigue such as those that result from illness, alcohol consumption, tiredness, or illness. StoneLock claimed that the implementation of this robust by several leading Fortune 100 companies proved that it worked perfectly, but this just doesn&rsquo;t add up.&nbsp; The Demographics of most Fortune 100 companies vary greatly from those of the Atlantic Towers residents, and few people report to work under the conditions of illness, fatigue, or intoxication that one may present at their home.</p>
<p><em><a href="https://arxiv.org/pdf/1804.03209">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</a> </em>is the paper by Peter Warden from Google Brain explaining the creation process for Speech Command Recognizer. In it, he exposes that Speech Command Recognizer has many of the same bias issues found in the StoneLock facial recognition technology.&nbsp; This technology, designed specifically to identify the voice of a primary user and respond accordingly, is built upon an initial data training set that was largely contributed to by anonymous volunteers and some paid contributors.&nbsp; He intentionally did not collect information about the race, age, gender, or ethnicity of the participants.&nbsp; While he admits that he focused primarily on American English speakers, he made no effort to ensure a mix of native and non-native speakers, nor was there any mention of efforts to include speakers of various regional accents of the English language or those whose speech is impaired for any number of reasons. Volunteers were collected primarily through social media, and the social media networks of Mr. Warden are likely highly representative of the tech field at large, i.e. primarily white and some Asian men residing in Silicon Valley or other tech and research hubs.</p>
<p>This is color-deafness.&nbsp; The lack of efforts toward inclusion means that if this technology works successfully for the privileged classes, Google will claim it to be successful, even if it fails when used by a large percentage of folks who don&rsquo;t fit into the demographic boxes of the privileged classes. As we don&rsquo;t yet know how the Teachable Machine will be implemented, we don&rsquo;t know how disastrous its malperformance for marginalized groups could be.</p>
<p>Dr. Buolamwimi sights yet another example in this chapter that should be a warning here. Canadian firm Winterlight Labs created a system that used machine learning to attempt to detect indications of Alzheimer&rsquo;s disease using voice recordings. The system was built using a dataset comprised primarily the voices of anglophone Canadians. But when francophone Canadians tried to use the system, they did not match the training data, and the system failed to match correctly.&nbsp; Using the Teachable Machine for musical applications led me to see one of the possible limitations it has.&nbsp; Dr. Buolamwimi&rsquo;s research has pointed out how these shortcomings of inclusion can prove disastrous for underprivileged people.&nbsp; Now is the time to expose the coded gaze in all of our work with AI, even if on the surface it does not seem connected.</p>
        <h1>Our Teachable Machine</h1>
        <h2>Our Trained Audio Model</h2>
        <p>Our audio model identifies pitch! Please allow our webpage to have microphone permissions to see the model in action.</p>
 		<iframe src="https://editor.p5js.org/dmoore33/full/vs-8c6vZC" width="600px" height="400px"></iframe>
	<p>
		<a href="https://github.com/egbecker/lis-500/blob/main/index.html">Click Here For Our Code</a>
	</p>
	 <p>
            <a href="https://teachablemachine.withgoogle.com/">Click Here to Access Teachable Machines</a>
        </p>
	    <ol></ol>
	    <footer>
            <p>
                <!-- IP info below -->
                Copyright Liz Becker and David Moore 2024<br />
                <a href="mailto:egbecker@wisc.edu" class="cornsilk">egbecker@wisc.edu</a>
                <a href="mailto:dmoore33@wisc.edu" class="cornsilk">dmoore33@wisc.edu</a>
            </p>
        </footer>
    </body>
</html>
