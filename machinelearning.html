<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Machine Learning</title>
        <link rel="stylesheet" href="styles.css" />
   		<script src="https://lis500-egbecker.ischool.wisc.edu/p5.js"></script>]
		   <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/latest/p5.min.js"></script>

    </head>
    <body>
        <header>
            <!-- using buttons in the nav bar for look and feel -->
            <ul>
                <li><a class="btn" href="https://lis500-egbecker.ischool.wisc.edu/">Home</a></li>
                <li><a class="btn" href="aboutus.html">About Us</a></li>
                <li><a class="btn" href="techhero.html">Our Tech Heroes</a></li>
                <li><a class="btn" href="resources.html">Implicit Bias Resources</a></li>
                <li><a class="btn" href="machinelearning.html">Machine Learning</a></li>
            </ul>
        </header>
        <h1>Machine Learning</h1>
        <p>
            Machine Learning (ML) is a subset of artificial intelligence (AI) that involves training computer algorithms
            to recognize patterns and make data-driven predictions or decisions without being explicitly programmed. At
            its core, ML enables systems to learn from past experiences, improving their performance over time as they
            process more data.
        </p>
        <p>
            The process begins with data collection, where raw datasets are gathered and prepared for analysis. This
            step often includes cleaning the data to remove errors, inconsistencies, or irrelevant information. The next
            step is feature selection, where key variables or attributes in the dataset are chosen to enhance model
            accuracy.
        </p>
        <p>
            Once the data is prepared, an ML algorithm is selected based on the problem type, such as classification,
            regression, or clustering. Algorithms like decision trees, neural networks, or support vector machines are
            then trained using a portion of the dataset, enabling them to identify relationships within the data.
        </p>
        <p>
            During training, the algorithm adjusts its parameters to minimize errors, improving its predictive
            performance. After training, the model is validated and tested with new data to ensure reliability and
            generalization. Finally, the model can be deployed to make real-world predictions or decisions, with its
            performance monitored over time for continuous improvement.
        </p>
        <h1>Our Project Statements</h1>
        <h2>Liz's Statement</h2>
        <p>
            Machine learning, a rapidly advancing branch of artificial intelligence, has captured the imagination of
            technologists and the general public alike. However, the foundational concepts behind AI can often seem out
            of reach for those without a technical background. Google's Teachable Machines platform bridges this gap,
            empowering anyone with basic web development skills and a creative mindset to understand and experiment with
            machine learning. For this project, we embarked on a journey to design and implement a machine learning
            algorithm, integrate it into a functional webpage, and critically reflect on our work's ethical and societal
            implications, inspired by Joy Buolamwini's <i>Unmasking AI</i>.
        </p>
        <p>
            This project was not just about theory; it was about hands-on experimentation and practical learning. By
            following the structured tutorial provided by Teachable Machines, David and I gained the practical skills to
            create and deploy an algorithm capable of classifying images or recognizing voices. We started by training
            the algorithm using a custom dataset. Using a simple webcam setup, we captured examples of the categories we
            wanted the machine to distinguish, ensuring the dataset was varied enough to provide robust training while
            remaining manageable within the constraints of the project. After training the model, we exported it and
            integrated it into a custom webpage using HTML, CSS, and JavaScript. Our completed webpage serves as both a
            demonstration of the algorithm in action and a reflection of our learning journey. Visitors to the page can
            interact with the trained model and observe how it classifies inputs in real-time. We embedded a video
            showcasing the algorithm's development and operation to complement this interactive element.
        </p>
        <p>
            As we worked through this project, the critical themes in <i>Unmasking AI</i> provided a vital lens for
            evaluating our work. Buolamwini's emphasis on bias in AI systems was particularly resonant. No matter how
            advanced, machine learning models are only as unbiased as the data they are trained on. Datasets can
            inadvertently reflect the prejudices and inequalities of the natural world, leading to algorithms that
            perpetuate these issues. This awareness led us to evaluate the dataset we created carefully. We aimed to
            include many examples to ensure the model was as inclusive and representative as possible. However, we
            recognized that true inclusivity requires ongoing vigilance, feedback, and refinement, emphasizing the
            importance of iterative development in AI design.
        </p>
        <p>
            Buolamwini's call for ethical responsibility in AI development also prompted us to reflect on the broader
            implications of our work. While this project is a relatively small-scale exercise, it mirrors the challenges
            of larger AI systems deployed worldwide. For example, even though our algorithm operates in a controlled
            environment, questions about its potential misuse, limitations, and design assumptions remained central to
            our discussions. These reflections deepened our understanding of the need for transparency and
            accountability in AI development, reassuring us and the audience of the responsible future of technology.
        </p>
        <p>
            The collaborative nature of this project added another layer of richness to the experience. Working as a
            group required us to navigate different perspectives, skills, and approaches to problem-solving. Both David
            and I brought unique strengths, from technical proficiency in coding to creative ideas for presenting the
            project on the webpage. This diversity of thought enhanced the quality of our work and mirrored the
            multidisciplinary approach that successful AI development often requires. Our collaboration underscored the
            importance of communication, adaptability, and shared responsibility, all essential skills in both academic
            and professional contexts.
        </p>
        <p>
            Another significant aspect of this project was the integration of creativity with technical implementation.
            Designing the algorithm and building the webpage required analytical thinking and artistic expression.
            Decisions about the webpage's layout, color scheme, and user experience were just as important as the
            technical accuracy of the algorithm itself. This interplay between creativity and technology highlighted the
            potential for AI to be a tool for solving problems and enabling new forms of artistic and cultural
            expression.
        </p>
        <p>
            This project was not just about our own learning and development; it was also about contributing to the
            broader community of learners and developers. By uploading our code to GitHub and documenting our process,
            we made a significant contribution to the open-source ecosystem. Our work is now available for others to
            learn from and build upon, aligning with the ethos of transparency and collaboration that Buolamwini
            advocates for. This project is a testament to our belief that AI development should be inclusive and
            participatory.
        </p>
        <p>
            The final stages of the project, including soliciting feedback and making iterative improvements,
            underscored the value of continual learning and refinement. Feedback from peers and instructors provided
            fresh perspectives on our work, revealing blind spots we had overlooked and suggesting ways to enhance the
            project's functionality and presentation. These iterative cycles of development and critique mirrored the
            real-world processes of AI research and development, where no system is ever truly finished, and improvement
            is a constant goal. This commitment to improvement should motivate and inspire all of us in the field of AI.
        </p>
        <p>
            In reflecting on the entire experience, several vital lessons emerge. First, machine learning is more opaque
            than it often seems; with the right tools and guidance, anyone can understand and experiment with it.
            Second, creating and deploying an algorithm is more about ethical considerations than technical precision.
            Buolamwini's work reminded us that every choice we make in designing an AI system carries implications for
            its fairness, inclusivity, and impact on society. Finally, this project's collaborative and creative aspects
            underscored the importance of bringing diverse perspectives and disciplines together to tackle complex
            challenges.
        </p>
        <p>
            This project represents a convergence of technical skills, ethical inquiry, and creative exploration. It is
            a testament to the power of accessible tools like Teachable Machines to democratize AI and empower
            individuals to engage with technology in meaningful ways. As we move forward, the lessons learned from this
            project will continue to inform our approach to AI, reminding us of the importance of curiosity,
            responsibility, and collaboration in shaping the future of technology.
        </p>
        <h2>David's Statement</h2>
        <p><strong>All 12 Notes but no Color; Employing Google&rsquo;s Teachable Machine with Musical Datasets</strong></p>
<p>We programmed our teachable machine to identify the 12 chromatic pitches of Western music. Play, sing, or whistle a note into your microphone and the machine will correctly identify it; When asked to perform higher-level musical analysis, the recognition of chordal quality, the machine failed. The journey to create this teachable machine was interesting and exposed some truths about my biases and those inherent in the machine. Through this process, understanding how the various problematic and harmful examples of AI algorithms and datasets presented in Joy Buolamwimi&rsquo;s <em>Unmasking AI </em>so easily found their way into common use. As Buolamwimi herself relates, &ldquo;as a computer scientist, I was trained to focus on the technical, not the ethical.&rdquo; The lack of ethical considerations is only compounded by the pressures of the competitive rush to be first in research and the marketplace.</p>
<p>&nbsp;</p>
<p><strong>Music Theory Background Information</strong></p>
<p>Our original plan was not to create a machine that identified single pitches accurately, but one that could identify the quality of the 4 types of chords most commonly found in Jazz/ Black American Music; Major 7, minor 7, Dominant 7, and Minor 7 flat 5 chords. Unlike isolated individual pitches, each of these chord types has a distinct sound that most students of contemporary music can identify without other contextual clues. &nbsp;Chordal quality is an important aspect of both <a href="https://iastate.pressbooks.pub/comprehensivemusicianship/chapter/6-1-diatonic-harmony-tutorial/">functional</a> and <a href="https://www.berklee.edu/berklee-today/fall-1999/Nonfunctional-Harmony">non-functional</a> harmony and is often compared to the use of color in the visual arts.&nbsp; It is determined by the intervallic relationships between the four notes of the various types of seventh chords. These notes are the first, third, fifth, and seventh scale degrees. For example, here are all the notes of the C major scale:</p>
<p>C, D, E, F, G, A, B</p>
<p>The C major 7 chord would be just the &nbsp;4 notes C, E, G, and B.&nbsp; These notes are all separated by an interval of either a major or minor third.&nbsp; In the case of a major 7 chord, the intervals are as follows:</p>
<p>C to E, Major third</p>
<p>E to G, Minor third</p>
<p>G to B, Major third</p>
<p>We can create a chord starting from each note of the same scale, and get at least one of each of our 4 chord types from any major scale. For example, if we arrange all the notes of the C Major scale to start on D, we get the following:</p>
<p>D, E, F, G, A, B, C</p>
<p>This is considered to be the mode of D Dorian, and from it we can derive a D Minor 7 chord, using the notes D, F, A, and C; The intervals between these chord tones are as follows:</p>
<p>D to F, Minor third</p>
<p>F to A, Minor Third</p>
<p>A to C, Minor Third</p>
<p>If we arrange the notes starting on the fifth scale degree of C major, the G note, we get the G Mixolydian mode:</p>
<p>G, A, B, C, D, E, F</p>
<p>The corresponding G Dominant 7 Chord, spelled G, B, D, and F, is structured on the following stacked thirds:</p>
<p>G to B, Major Third</p>
<p>B to D, Minor Third</p>
<p>D to F, Minor Third</p>
<p>To get a Minor 7 flat 5 chord, we need to start at the 7<sup>th</sup> scale degree of C Major, thereby attaining the B Locrian mode:</p>
<p>B, C, D, E, F, G, A</p>
<p>The B Minor 7 flat 5 chord, spelled B, D, F, and A, has the following intervallic relationships:</p>
<p>B to D, Minor Third</p>
<p>D to F, Minor Third</p>
<p>F to A, Major Third</p>
<p>When learning to identify chordal qualities, students are often encouraged to find feelings or sensory adjectives that each chordal type seems to convey to them personally. I hear Major 7 chords as cool, Minor 7 chords as dark and warm, Dominant 7 chords as bright, and Minor 7 flat 5 as cold and mysterious. These feelings associations are not uncommon, and there is literature that uses similar adjectives to describe the <em>feeling </em>of these chordal qualities similarly. There have even been some <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5717663/">questionable attempts at codifying the emotional content of chordal qualities</a>. &nbsp;These feelings are likely more reflective of the values and traditions of the dominant culture than they are universally true. There are plenty of examples outside and inside Western music of harmonic structures being associated with music that runs antithetical to this, such as the Blues, which in its most essential form is built almost exclusively on dominant chords, and the popular and folkloric music of the Balkans and East Africa, which are almost exclusively built on minor modes, yet are often songs of celebration in the former or spiritual praise or romantic longing in the later. Regardless of the specific associations, you would be hard-pressed to find anyone who doesn&rsquo;t have some type of feeling associations with these rather complex chordal structures, thereby making them easy to identify through emotional imprint, even for the least knowledgeable of students.</p>
<p>Alternatively, identifying individual pitches in isolation is very difficult, even for the most seasoned of professional musicians and music educators.&nbsp; This ability, often called perfect pitch, is very rare in humans and imperfect, at best.&nbsp; Most human musicians rely on relative pitch, which is more about hearing and understanding the relationships between harmonic and melodic intervals than about identifying them in isolation.&nbsp; A skilled musician with good relative pitch can learn a melody or a harmonic sequence from a recording without touching a musical instrument, but they will need to play their instrument as a point of reference to determine what key the music is in. Perfect pitch can actually prove detrimental to some musicians, as their hearing can change over time, <a href="https://www.youtube.com/watch?v=3rx08qWtFak">and they can begin to hear things out of tune</a>.</p>
<p>The Teachable &nbsp;Machine can easily identify an individual pitch or note because it simply has to identify a specific frequency and then respond with the name of that frequency as it was taught.&nbsp; While the timbre of different instruments or voices will color the note differently, the fundamental pitch is always the same, as are the potential harmonics.&nbsp; &nbsp;Identifying pitches this way is easily accomplished with much simpler technology than this teachable machine.&nbsp; A great example of this is an <a href="https://www.fwordmag.com/single-post/how-does-a-guitar-tuner-work">electric guitar tuner</a>.&nbsp; Technology like this has its foundations in the <a href="https://en.wikipedia.org/wiki/History_of_the_oscilloscope">earliest oscilloscopes</a>, which date to the late 19<sup>th</sup> century.&nbsp; The Google Teachable machine analyzes waveforms, similarly to the oscilloscope or a guitar tuner. The big difference is that the machine applies meaning to that information, in this case, that meaning is a note name.&nbsp;</p>
<p><strong>Data sample creation process</strong></p>
<p><strong>Pt 1: The Chord Quality Machine</strong></p>
<p>Operating from the perspective of a human musician who does not have perfect pitch I thought that building a machine that could learn what different chord qualities sound like and identify them would be relatively easy.&nbsp; Using my vibraphone, audio interface, and one Blue Bottle microphone, I was ready to capture some audio samples. I decided to put some basic parameters in place to build the initial data set. I went through several unsuccessful iterations, below are the parameters for the final version I created:</p>
<ol>
<li>I created 5 classes for the Teachable Machine; the 4 types of chords and a background noise class.</li>
<li>I recorded all 4 types of chords in root position (1,3,5,7 chord spelling) in all 12 keys. I decided not to record other common voicings of these chords or add harmonic tensions, even though in actual musical practice these would be commonplace.&nbsp; I was determined to get the machine to first recognize these basic versions of the chords, and then add those variations later if it was successful.</li>
<li>Each chord was played by me on the vibraphone and recorded with the sustain pedal suppressed, allowing the chord to ring out. I recorded each chord for 8 seconds, which in turn were broken up into 8, 1 second samples. This results in a total of 384 samples in total.</li>
<li>The datasets were fed through the teachable machine for 300 epochs</li>
</ol>
<p>&nbsp;</p>
<p>Unfortunately, the machine misidentified the chord qualities nearly every time</p>
<p><strong>A Theory on Why the Chord Quality Identifier Failed</strong></p>
<p>&nbsp;My faulty assumption was that the aspects of aural training that humans find easiest to master would be the same ones that the machine could easily identify.&nbsp; This thesis proved incorrect. The teachable machine is powered by transfer learning, and built on<a href="https://github.com/tensorflow/tfjs-models/tree/master/speech-commands"> Speech Command Recognizer.</a> This model is built upon TensorFlow.js, and was designed specifically for simple speech recognition tasks:</p>
<p>&ldquo;Its primary goal is to provide a way to build and test small models that detect when a single word is spoken, from a set of ten or fewer target words, with as few false positives as possible from background noise or unrelated speech. This task is often known as keyword spotting.&rdquo;</p>
<p>Keyword spotting is the voice recognition used to call the attention of voice-controlled devices and applications, such as &ldquo;Hey Siri,&rdquo; or &ldquo;Hey Google.&rdquo; The human voice, under the normal circumstances of speech, only produces one pitch at a time on vowel sounds, and non-pitched articulations on consonant sounds.&nbsp; The machine&rsquo;s initial datasets were linear series of singular frequencies and non-pitched sounds, in 1-second groupings.&nbsp; The simplicity and lack of context of these pitches is what makes their recognition so challenging to human pitch memory. Similarly, the complexity of the chord qualities makes them very easy for human ears to classify, but nearly impossible for this machine-learning model to recognize as distinct classes.&nbsp; My assumption that machine learning would find the same tasks easy that humans do was totally flipped on its head in this process.&nbsp; The implications here suggest that the biases of both the Teachable Machine's creators and the members of the tech community using this and similar technology can lead to problematic and even dangerous outcomes. Taking a closer look at the creation of the Teachable Machine&rsquo;s underlying small language learning model, Speech Command Recognizer, reveals the potential and high probability of the ghosts of the bias in this machine.</p>
<p><strong>Data sample creation process</strong></p>
<p><strong>Pt 2: The Pitch Identifying Machine</strong></p>
<p>When the Teachable Machine failed to recognize chord quality, I wanted to find something it could do successfully. I inverted my thinking; if it failed at a task most musicians find easy, how would it do on a task most musicians find extremely difficult?&nbsp; &nbsp;&nbsp;Bearing in mind that the pitch memory of the machine was absolute so long as I had a class for all 12 pitches, I thought that success was highly likely.&nbsp; Here is how I created my datasets to teach the machine perfect pitch:</p>
<ol>
<li>I created 13 classes; one for each of the 12 pitches and one for background noise</li>
<li>For each of the 12 note classes I played the pitch in all available octaves on my vibraphone. The vibraphone is a 3-octave instrument, starting with the F above middle C and terminating at the F 4 octaves above that. This means that there are 4 F&rsquo;s, and three of every other pitch sampled, totaling 37 notes.</li>
<li>Each note was played by me on the vibraphone and recorded with the sustain pedal suppressed, allowing it to ring out. I recorded each chord for 8 seconds, which in turn were broken up into 8, 1 second samples. This means there is a total of 296, 1-second samples.</li>
<li>The datasets were fed through the teachable machine for 100 epochs</li>
</ol>
<p>&nbsp;</p>
<p><strong>A note on pitch and tuning:</strong> This machine, being based on the well-tempered system that all keyboard instruments are tuned to, does not account for the slight differences in pitch that a vocal or string ensemble would employ.&nbsp; For example, on a keyboard, the notes Bb and A# are enharmonically the same but will be slightly different than each other when performed by a vocal ensemble.&nbsp; For more on this please visit the Wikipedia article about musical temperament <a href="https://en.wikipedia.org/wiki/Musical_temperament">here</a>.</p>
<p>&nbsp;</p>
<p><strong>The Coded Gaze</strong></p>
<p><em>&ldquo;</em><em>In my work, I use the coded gaze term as a reminder that the machines we build reflect the priorities, preferences, and even prejudices of those who have the power to shape technology. The coded gaze does not have to be explicit to do the job of oppression. Like systemic forms of oppression, including patriarchy and white supremacy, it is programmed into the fabric of society. Without intervention, those who have held power in the past continue to pass that power to those who are most like them. This does not have to be intentional to have a negative impact. The task ahead of me was to see if I could find compelling evidence showing the coded gaze at work.&rdquo;</em></p>
<p>
Joy Buolamwimi<em>, Unmasking AI: My Mission to Protect What is Human in a World of Machines</em>
</p>https://lis500-egbecker.ischool.wisc.edu/p5.js
<p>In <em>Unmasking AI, </em>Dr. Joy Buolamwimi exposes and dissects the inherent bias found within the algorithms and data collection methodologies that power much of the AI that is currently being used for commercial and governmental applications.&nbsp; She is first exposed to the coded gaze when working on a graduate school project that requires her to employ code used for existing facial recognition systems. The system does not recognize her dark complexion as a face, and she is forced to use a white Halloween mask to use and test her project.&nbsp; Digging deeper, she uncovers that the datasets that the AI has learned from reflect the biases of its creators.&nbsp; As she unpacks this, it is discovered that this is a consistent problem across most of the machine learning systems in use.&nbsp; Throughout the book, Dr. Buolamwimi works tirelessly to expose the potential dangers it poses to underrepresented groups, particularly women and people of color.&nbsp;</p>
<p>In chapter 16 of the book, entitled <em>BROOKLYN TENANTS, </em>Dr. Buolamwimi is recruited to assist a legal team that represents the residents of Atlantic Towers, a Brooklyn, NY apartment complex.&nbsp; The management company that operates the building is planning on replacing the existing keyless fob entry system with a facial recognition entry system created by a company called StoneLock. The system uses the heatmap of an individual&rsquo;s face to allow entry to the building. The firm claims that it does not differentiate the gender, sex, or color of the person trying to enter the building because it &ldquo;merely reads data points on a person&rsquo;s face and assigns a number.&rdquo; The legal team asks Dr. Buolamwimi how this system may infringe on the rights of the tenants of Atlantic Towers, and possibly create difficulty accessing their homes.</p>
<p>The tenants had not agreed to having their faces used for biometric entry prior to moving into Atlantic Towers. They had no way of ensuring that their biometric data wasn&rsquo;t being sold or turned over to law enforcement. Dr. Buolamwimi&rsquo;s work had already exposed the lack of biometric information available about women of color, and that population was the large majority of tenants; &ldquo;The vast majority of the tenants belonged to one or more of the groups that have the highest failures in the U.S government-sponsored studies [<em>sic</em>] &hellip; that examined the accuracy of facial recognition technologies.&rdquo;</p>
<p>StoneLock didn&rsquo;t make clear the demographic and phenotypic makeup of the training tests they used to develop their product, and Black and Brown folks were underrepresented in the publicly available datasets at the time.&nbsp; Furthermore, there were specific challenges of the near-infrared facial recognition methods that StoneLock was using.&nbsp; The accuracy of the system could be compromised if people trying to enter using the system presented signs of physical or emotional fatigue such as those that result from illness, alcohol consumption, tiredness, or illness. StoneLock claimed that the implementation of this robust by several leading Fortune 100 companies proved that it worked perfectly, but this just doesn&rsquo;t add up.&nbsp; The Demographics of most Fortune 100 companies vary greatly from those of the Atlantic Towers residents, and few people report to work under the conditions of illness, fatigue, or intoxication that one may present at their home.</p>
<p><em><a href="https://arxiv.org/pdf/1804.03209">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</a> </em>is the paper by Peter Warden from Google Brain explaining the creation process for Speech Command Recognizer. In it, he exposes that Speech Command Recognizer has many of the same bias issues found in the StoneLock facial recognition technology.&nbsp; This technology, designed specifically to identify the voice of a primary user and respond accordingly, is built upon an initial data training set that was largely contributed to by anonymous volunteers and some paid contributors.&nbsp; He intentionally did not collect information about the race, age, gender, or ethnicity of the participants.&nbsp; While he admits that he focused primarily on American English speakers, he made no effort to ensure a mix of native and non-native speakers, nor was there any mention of efforts to include speakers of various regional accents of the English language or those whose speech is impaired for any number of reasons. Volunteers were collected primarily through social media, and the social media networks of Mr. Warden are likely highly representative of the tech field at large, i.e. primarily white and some Asian men residing in Silicon Valley or other tech and research hubs.</p>
<p>This is color-deafness.&nbsp; The lack of efforts toward inclusion means that if this technology works successfully for the privileged classes, Google will claim it to be successful, even if it fails when used by a large percentage of folks who don&rsquo;t fit into the demographic boxes of the privileged classes. As we don&rsquo;t yet know how the Teachable Machine will be implemented, we don&rsquo;t know how disastrous its malperformance for marginalized groups could be.</p>
<p>Dr. Buolamwimi sights yet another example in this chapter that should be a warning here. Canadian firm Winterlight Labs created a system that used machine learning to attempt to detect indications of Alzheimer&rsquo;s disease using voice recordings. The system was built using a dataset comprised primarily the voices of anglophone Canadians. But when francophone Canadians tried to use the system, they did not match the training data, and the system failed to match correctly.&nbsp; Using the Teachable Machine for musical applications led me to see one of the possible limitations it has.&nbsp; Dr. Buolamwimi&rsquo;s research has pointed out how these shortcomings of inclusion can prove disastrous for underprivileged people.&nbsp; Now is the time to expose the coded gaze in all of our work with AI, even if on the surface it does not seem connected.</p>
        <h1>Our Teachable Machine</h1>
        <h2>Our Trained Audio Model</h2>
        <p>Our audio model identifies pitch! Please allow our webpage to have microphone permissions to see the model in action.</p>
 		<iframe src="https://editor.p5js.org/dmoore33/full/vs-8c6vZC" width="600px" height="400px"></iframe>
</html>
